{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "074b67b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>original_text</th>\n",
       "      <th>processed_text</th>\n",
       "      <th>nmf_topic</th>\n",
       "      <th>topic_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>From: lerxst@wam.umd.edu (where's my thing)\\nS...</td>\n",
       "      <td>thing car nntp_poste host rac_wam park line wo...</td>\n",
       "      <td>6</td>\n",
       "      <td>0.150893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>From: guykuo@carson.u.washington.edu (Guy Kuo)...</td>\n",
       "      <td>final call summary final call si clock report ...</td>\n",
       "      <td>9</td>\n",
       "      <td>0.022004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>From: twillis@ec.ecn.purdue.edu (Thomas E Will...</td>\n",
       "      <td>question organization purdue_university engine...</td>\n",
       "      <td>9</td>\n",
       "      <td>0.030069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>From: jgreen@amber (Joe Green)\\nSubject: Re: W...</td>\n",
       "      <td>division line distribution_world nntp_poste ho...</td>\n",
       "      <td>9</td>\n",
       "      <td>0.032953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>From: jcm@head-cfa.harvard.edu (Jonathan McDow...</td>\n",
       "      <td>question organization smithsonian_astrophysica...</td>\n",
       "      <td>9</td>\n",
       "      <td>0.016379</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                      original_text  \\\n",
       "0           0  From: lerxst@wam.umd.edu (where's my thing)\\nS...   \n",
       "1           1  From: guykuo@carson.u.washington.edu (Guy Kuo)...   \n",
       "2           2  From: twillis@ec.ecn.purdue.edu (Thomas E Will...   \n",
       "3           3  From: jgreen@amber (Joe Green)\\nSubject: Re: W...   \n",
       "4           4  From: jcm@head-cfa.harvard.edu (Jonathan McDow...   \n",
       "\n",
       "                                      processed_text nmf_topic  topic_score  \n",
       "0  thing car nntp_poste host rac_wam park line wo...         6     0.150893  \n",
       "1  final call summary final call si clock report ...         9     0.022004  \n",
       "2  question organization purdue_university engine...         9     0.030069  \n",
       "3  division line distribution_world nntp_poste ho...         9     0.032953  \n",
       "4  question organization smithsonian_astrophysica...         9     0.016379  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gensim\n",
    "import nltk\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "data = pd.read_csv('nmf_result.csv')\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aab9817d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanText(text):\n",
    "    cleaned = re.sub(\"[^a-zA-Z0-9']\",\" \",text)\n",
    "    return cleaned.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0bc2debf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['thing car nntp poste host rac wam park line wonder enlighten car see day door sport car look late early call door really small addition front bumper separate rest body know tellme model name engine spec year production car make history info funky look car mail thank bring neighborhood lerxst',\n",
       " 'final call summary final call si clock report keyword si acceleration clock upgrade article line nntp poste host fair number brave soul upgrade si clock oscillator share experience poll send brief message detail experience procedure top speed attain cpu rate speed add card adapter heat sink hour usage day floppy disk functionality floppy especially request summarize next day add network knowledge base do clock upgrade answer poll thank',\n",
       " 'question organization purdue university engineering computer network line well folk finally give ghost weekend start life way back sooo m market new machine bit soon intend m look pick powerbook maybe bunch question hopefully answer know dirt next round powerbook introduction expect d hear suppose make appearence summer hear anymore access macleak wonder info hear rumor price drop powerbook line one duo go recently s impression display probably swing get disk rather really feel much well display yea look great store really good solicit opinion people day day worth take disk size money hit get active display realize real subjective question ve play machine computer store breifly figured opinion actually use machine daily prove helpful well hellcat perform thank bunch advance info email ill post summary news read time premium final corner conviction dangerous enemie truth lie nietzsche',\n",
       " 'division line distribution world nntp poste host amber write write article know chip far low level stuff go look pretty nice get quadrilateral fill command require point weitek address phone number i get information chip computer division thing really scare person sense winter']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x,y = np.asarray(data[\"processed_text\"]),np.asarray(data[\"nmf_topic\"])\n",
    "\n",
    "x_cleaned = [cleanText(t) for t in x]\n",
    "x_cleaned[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10c1cfa6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['thing',\n",
       " 'car',\n",
       " 'nntp',\n",
       " 'poste',\n",
       " 'host',\n",
       " 'rac',\n",
       " 'wam',\n",
       " 'park',\n",
       " 'line',\n",
       " 'wonder',\n",
       " 'enlighten',\n",
       " 'car',\n",
       " 'see',\n",
       " 'day',\n",
       " 'door',\n",
       " 'sport',\n",
       " 'car',\n",
       " 'look',\n",
       " 'late',\n",
       " 'early',\n",
       " 'call',\n",
       " 'door',\n",
       " 'really',\n",
       " 'small',\n",
       " 'addition',\n",
       " 'front',\n",
       " 'bumper',\n",
       " 'separate',\n",
       " 'rest',\n",
       " 'body',\n",
       " 'know',\n",
       " 'tellme',\n",
       " 'model',\n",
       " 'name',\n",
       " 'engine',\n",
       " 'spec',\n",
       " 'year',\n",
       " 'production',\n",
       " 'car',\n",
       " 'make',\n",
       " 'history',\n",
       " 'info',\n",
       " 'funky',\n",
       " 'look',\n",
       " 'car',\n",
       " 'mail',\n",
       " 'thank',\n",
       " 'bring',\n",
       " 'neighborhood',\n",
       " 'lerxst']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_tokenized = [[w for w in sentence.split(\" \") if w != \"\"] for sentence in x_cleaned]\n",
    "x_tokenized[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "446e9d9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This process took 5.76 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Now we'll create our model \n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "model = gensim.models.Word2Vec(x_tokenized,\n",
    "                 size=100\n",
    "                 # Size is the length of our vector.\n",
    "                )\n",
    "\n",
    "end = round(time.time()-start,2)\n",
    "print(\"This process took\",end,\"seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "59fdb636",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('maintain', 0.7264387607574463),\n",
       " ('customer', 0.7227741479873657),\n",
       " ('fee', 0.7188107967376709),\n",
       " ('directly', 0.7004418969154358),\n",
       " ('interest', 0.6920909881591797),\n",
       " ('existant', 0.6852169632911682),\n",
       " ('business', 0.6830903887748718),\n",
       " ('redistribute', 0.6816473007202148),\n",
       " ('service', 0.6813581585884094),\n",
       " ('privacy', 0.6771520376205444)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(\"free\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0cf1fc9",
   "metadata": {},
   "source": [
    "## Writing A Class To Create Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a184a140",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequencer():\n",
    "    \n",
    "    def __init__(self,\n",
    "                 all_words,\n",
    "                 max_words,\n",
    "                 seq_len,\n",
    "                 embedding_matrix\n",
    "                ):\n",
    "        \n",
    "        self.seq_len = seq_len\n",
    "        self.embed_matrix = embedding_matrix\n",
    "        \"\"\"\n",
    "        temp_vocab = Vocab which has all the unique words\n",
    "        self.vocab = Our last vocab which has only most used N words.\n",
    "    \n",
    "        \"\"\"\n",
    "        temp_vocab = list(set(all_words))\n",
    "        self.vocab = []\n",
    "        self.word_cnts = {}\n",
    "        \"\"\"\n",
    "        Now we'll create a hash map (dict) which includes words and their occurencies\n",
    "        \"\"\"\n",
    "        for word in temp_vocab:\n",
    "            # 0 does not have a meaning, you can add the word to the list\n",
    "            # or something different.\n",
    "            count = len([0 for w in all_words if w == word])\n",
    "            self.word_cnts[word] = count\n",
    "            counts = list(self.word_cnts.values())\n",
    "            indexes = list(range(len(counts)))\n",
    "        \n",
    "        # Now we'll sort counts and while sorting them also will sort indexes.\n",
    "        # We'll use those indexes to find most used N word.\n",
    "        cnt = 0\n",
    "        while cnt + 1 != len(counts):\n",
    "            cnt = 0\n",
    "            for i in range(len(counts)-1):\n",
    "                if counts[i] < counts[i+1]:\n",
    "                    counts[i+1],counts[i] = counts[i],counts[i+1]\n",
    "                    indexes[i],indexes[i+1] = indexes[i+1],indexes[i]\n",
    "                else:\n",
    "                    cnt += 1\n",
    "        \n",
    "        for ind in indexes[:max_words]:\n",
    "            self.vocab.append(temp_vocab[ind])\n",
    "                    \n",
    "    def textToVector(self,text):\n",
    "        # First we need to split the text into its tokens and learn the length\n",
    "        # If length is shorter than the max len we'll add some spaces (100D vectors which has only zero values)\n",
    "        # If it's longer than the max len we'll trim from the end.\n",
    "        tokens = text.split()\n",
    "        len_v = len(tokens)-1 if len(tokens) < self.seq_len else self.seq_len-1\n",
    "        vec = []\n",
    "        for tok in tokens[:len_v]:\n",
    "            try:\n",
    "                vec.append(self.embed_matrix[tok])\n",
    "            except Exception as E:\n",
    "                pass\n",
    "        \n",
    "        last_pieces = self.seq_len - len(vec)\n",
    "        for i in range(last_pieces):\n",
    "            vec.append(np.zeros(100,))\n",
    "        \n",
    "        return np.asarray(vec).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "019e1e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequencer = Sequencer(all_words = [token for seq in x_tokenized for token in seq],\n",
    "              max_words = 1200,\n",
    "              seq_len = 15,\n",
    "              embedding_matrix = model.wv\n",
    "             )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b371028a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.23834062, 0.86124551, 0.09608242, ..., 0.        , 0.        ,\n",
       "       0.        ])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_vec = sequencer.textToVector(\"i am in love with you\")\n",
    "test_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "175d3847",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1500,)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_vec.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705222c7",
   "metadata": {},
   "source": [
    "## PCA (Principal Component Analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7722cf3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11314, 1500)\n"
     ]
    }
   ],
   "source": [
    "x_vecs = np.asarray([sequencer.textToVector(\" \".join(seq)) for seq in x_tokenized])\n",
    "print(x_vecs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d7d904d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum of variance ratios:  0.7076590097263021\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca_model = PCA(n_components=50)\n",
    "pca_model.fit(x_vecs)\n",
    "print(\"Sum of variance ratios: \",sum(pca_model.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "77f7adf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11314, 50)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_comps = pca_model.transform(x_vecs)\n",
    "x_comps.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9df2b578",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0': 0,\n",
       " '1': 1,\n",
       " '10': 2,\n",
       " '11': 3,\n",
       " '12': 4,\n",
       " '13': 5,\n",
       " '14': 6,\n",
       " '15': 7,\n",
       " '16': 8,\n",
       " '17': 9,\n",
       " '18': 10,\n",
       " '19': 11,\n",
       " '2': 12,\n",
       " '3': 13,\n",
       " '4': 14,\n",
       " '5': 15,\n",
       " '6': 16,\n",
       " '7': 17,\n",
       " '8': 18,\n",
       " '9': 19,\n",
       " 'None': 20}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_map = {cat:index for index,cat in enumerate(np.unique(y))}\n",
    "y_prep = np.asarray([label_map[l] for l in y])\n",
    "\n",
    "label_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a9ac6a6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['6', '9', '9', ..., '9', '13', '9'], dtype=object)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6e11d51d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9051, 50)\n",
      "(2263, 50)\n",
      "(9051,)\n",
      "(2263,)\n"
     ]
    }
   ],
   "source": [
    "x_train,x_test,y_train,y_test = train_test_split(x_comps,y,test_size=0.2,random_state=42)\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54751020",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time() \n",
    "\n",
    "svm_classifier = SVC(kernel=\"linear\", C=1000)\n",
    "svm_classifier.fit(x_train,y_train)\n",
    "\n",
    "end = time.time()\n",
    "process = round(end-start,2)\n",
    "print(\"Support Vector Machine Classifier has fitted, this process took {} seconds\".format(process))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2716d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_classifier.score(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78182913",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier()\n",
    "rfc.fit(x_train,y_train)\n",
    "print(\"Score of RFC\",rfc.score(x_test,y_test))\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(x_train,y_train)\n",
    "print(\"Score of LogReg\",logreg.score(x_test,y_test))\n",
    "\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(x_train,y_train)\n",
    "print(\"Score of GaussianNB\",gnb.score(x_test,y_test))\n",
    "\n",
    "bnb = BernoulliNB()\n",
    "bnb.fit(x_train,y_train)\n",
    "print(\"Score of BernoulliNB\",bnb.score(x_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3199250",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
